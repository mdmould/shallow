{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77a4bc5-6dfa-4b28-a840-66836eb702ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from corner import corner\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa1ee9b-e536-4194-ac3e-9f8518215fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerlaw(x, slope, lo, hi):\n",
    "    \n",
    "    return (\n",
    "        (x >= lo) * (x <= hi)\n",
    "        * x**slope\n",
    "        * (slope+1) / (hi**(slope+1) - lo**(slope+1))\n",
    "        )\n",
    "\n",
    "def sample_powerlaw(n_samples, slope, lo, hi):\n",
    "    \n",
    "    x = np.random.uniform(size=n_samples)\n",
    "    \n",
    "    return (lo**(slope+1) + x * (hi**(slope+1) - lo**(slope+1)))**(1/(slope+1))\n",
    "\n",
    "def gaussian(x, mean=0., std=1.):\n",
    "    \n",
    "    return np.exp(-(x-mean)**2 / (2*std**2)) / (std * (2*np.pi)**.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d825d4-e873-4a21-ba3d-70d5c5abdf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_slopes = 10\n",
    "n_dim = 2\n",
    "n_samples = 10000\n",
    "lo = 0\n",
    "hi = 1\n",
    "\n",
    "slopes = np.linspace(0, 5, n_slopes)\n",
    "data = sample_powerlaw([n_slopes, n_samples, n_dim], slopes[:, None, None], lo, hi)\n",
    "\n",
    "slopes.shape, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7da2461-f7b0-4293-96b5-57ccd3ffbc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "corner(data[i]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ac4b0-473f-44df-a47b-d35a1847f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes = np.repeat(slopes[:, None, None], n_samples, axis=1)\n",
    "slopes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c775129-6f38-4c57-8b8a-a56794e16c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes = slopes.reshape(-1, 1)\n",
    "data = data.reshape(-1, n_dim)\n",
    "slopes.shape, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb67ef56-1eec-4c8d-9a5b-935f1ee832c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm:\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \n",
    "        self.mean = np.mean(data, axis=0)\n",
    "        self.std = np.std(data, axis=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return (x - self.mean) / self.std\n",
    "    \n",
    "    def inverse(self, y):\n",
    "        \n",
    "        return y * self.std + self.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2433a820-a560-41f6-9f68-e59aeaf9a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = False\n",
    "\n",
    "if normalize:\n",
    "    slopes_norm = Norm(slopes)\n",
    "    slopes = slopes_norm.forward(slopes)\n",
    "    data_norm = Norm(data)\n",
    "    data = data_norm.forward(data)\n",
    "    \n",
    "    i = 0\n",
    "    corner(data[n_samples*i:n_samples*(i+1)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254fd4b2-893e-4a5a-8f20-ec7a267aa0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pytorch_flows as fnn\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad7af8b-9e6d-4657-a679-6222b028fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ikostrikov/pytorch-flows\n",
    "\n",
    "import math\n",
    "import types\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.linalg\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def get_mask(in_features, out_features, in_flow_features, mask_type=None):\n",
    "    \"\"\"\n",
    "    mask_type: input | None | output\n",
    "    \n",
    "    See Figure 1 for a better illustration:\n",
    "    https://arxiv.org/pdf/1502.03509.pdf\n",
    "    \"\"\"\n",
    "    if mask_type == 'input':\n",
    "        in_degrees = torch.arange(in_features) % in_flow_features\n",
    "    else:\n",
    "        in_degrees = torch.arange(in_features) % (in_flow_features - 1)\n",
    "\n",
    "    if mask_type == 'output':\n",
    "        out_degrees = torch.arange(out_features) % in_flow_features - 1\n",
    "    else:\n",
    "        out_degrees = torch.arange(out_features) % (in_flow_features - 1)\n",
    "\n",
    "    return (out_degrees.unsqueeze(-1) >= in_degrees.unsqueeze(0)).float()\n",
    "\n",
    "\n",
    "class MaskedLinear(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 out_features,\n",
    "                 mask,\n",
    "                 cond_in_features=None,\n",
    "                 bias=True):\n",
    "        super(MaskedLinear, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        if cond_in_features is not None:\n",
    "            self.cond_linear = nn.Linear(\n",
    "                cond_in_features, out_features, bias=False)\n",
    "\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "    def forward(self, inputs, cond_inputs=None):\n",
    "        output = F.linear(inputs, self.linear.weight * self.mask,\n",
    "                          self.linear.bias)\n",
    "        if cond_inputs is not None:\n",
    "            output += self.cond_linear(cond_inputs)\n",
    "        return output\n",
    "\n",
    "nn.MaskedLinear = MaskedLinear\n",
    "\n",
    "\n",
    "class MADE(nn.Module):\n",
    "    \"\"\" An implementation of MADE\n",
    "    (https://arxiv.org/abs/1502.03509).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_inputs,\n",
    "                 num_hidden,\n",
    "                 num_cond_inputs=None,\n",
    "                 act='relu',\n",
    "                 pre_exp_tanh=False):\n",
    "        super(MADE, self).__init__()\n",
    "\n",
    "        activations = {'relu': nn.ReLU, 'sigmoid': nn.Sigmoid, 'tanh': nn.Tanh}\n",
    "        act_func = activations[act]\n",
    "\n",
    "        input_mask = get_mask(\n",
    "            num_inputs, num_hidden, num_inputs, mask_type='input')\n",
    "        hidden_mask = get_mask(num_hidden, num_hidden, num_inputs)\n",
    "        output_mask = get_mask(\n",
    "            num_hidden, num_inputs * 2, num_inputs, mask_type='output')\n",
    "\n",
    "        self.joiner = nn.MaskedLinear(num_inputs, num_hidden, input_mask,\n",
    "                                      num_cond_inputs)\n",
    "\n",
    "        self.trunk = nn.Sequential(act_func(),\n",
    "                                   nn.MaskedLinear(num_hidden, num_hidden,\n",
    "                                                   hidden_mask), act_func(),\n",
    "                                   nn.MaskedLinear(num_hidden, num_inputs * 2,\n",
    "                                                   output_mask))\n",
    "\n",
    "    def forward(self, inputs, cond_inputs=None, mode='direct'):\n",
    "        if mode == 'direct':\n",
    "            h = self.joiner(inputs, cond_inputs)\n",
    "            m, a = self.trunk(h).chunk(2, 1)\n",
    "            u = (inputs - m) * torch.exp(-a)\n",
    "            return u, -a.sum(-1, keepdim=True)\n",
    "\n",
    "        else:\n",
    "            x = torch.zeros_like(inputs)\n",
    "            for i_col in range(inputs.shape[1]):\n",
    "                h = self.joiner(x, cond_inputs)\n",
    "                m, a = self.trunk(h).chunk(2, 1)\n",
    "                x[:, i_col] = inputs[:, i_col] * torch.exp(\n",
    "                    a[:, i_col]) + m[:, i_col]\n",
    "            return x, -a.sum(-1, keepdim=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "class BatchNormFlow(nn.Module):\n",
    "    \"\"\" An implementation of a batch normalization layer from\n",
    "    Density estimation using Real NVP\n",
    "    (https://arxiv.org/abs/1605.08803).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, momentum=0.0, eps=1e-5):\n",
    "        super(BatchNormFlow, self).__init__()\n",
    "\n",
    "        self.log_gamma = nn.Parameter(torch.zeros(num_inputs))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_inputs))\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "        self.register_buffer('running_mean', torch.zeros(num_inputs))\n",
    "        self.register_buffer('running_var', torch.ones(num_inputs))\n",
    "\n",
    "    def forward(self, inputs, cond_inputs=None, mode='direct'):\n",
    "        if mode == 'direct':\n",
    "            if self.training:\n",
    "                self.batch_mean = inputs.mean(0)\n",
    "                self.batch_var = (\n",
    "                    inputs - self.batch_mean).pow(2).mean(0) + self.eps\n",
    "\n",
    "                self.running_mean.mul_(self.momentum)\n",
    "                self.running_var.mul_(self.momentum)\n",
    "\n",
    "                self.running_mean.add_(self.batch_mean.data *\n",
    "                                       (1 - self.momentum))\n",
    "                self.running_var.add_(self.batch_var.data *\n",
    "                                      (1 - self.momentum))\n",
    "\n",
    "                mean = self.batch_mean\n",
    "                var = self.batch_var\n",
    "            else:\n",
    "                mean = self.running_mean\n",
    "                var = self.running_var\n",
    "\n",
    "            x_hat = (inputs - mean) / var.sqrt()\n",
    "            y = torch.exp(self.log_gamma) * x_hat + self.beta\n",
    "            return y, (self.log_gamma - 0.5 * torch.log(var)).sum(\n",
    "                -1, keepdim=True)\n",
    "        else:\n",
    "            if self.training:\n",
    "                mean = self.batch_mean\n",
    "                var = self.batch_var\n",
    "            else:\n",
    "                mean = self.running_mean\n",
    "                var = self.running_var\n",
    "\n",
    "            x_hat = (inputs - self.beta) / torch.exp(self.log_gamma)\n",
    "\n",
    "            y = x_hat * var.sqrt() + mean\n",
    "\n",
    "            return y, (-self.log_gamma + 0.5 * torch.log(var)).sum(\n",
    "                -1, keepdim=True)\n",
    "        \n",
    "        \n",
    "class Reverse(nn.Module):\n",
    "    \"\"\" An implementation of a reversing layer from\n",
    "    Density estimation using Real NVP\n",
    "    (https://arxiv.org/abs/1605.08803).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs):\n",
    "        super(Reverse, self).__init__()\n",
    "        self.perm = np.array(np.arange(0, num_inputs)[::-1])\n",
    "        self.inv_perm = np.argsort(self.perm)\n",
    "\n",
    "    def forward(self, inputs, cond_inputs=None, mode='direct'):\n",
    "        if mode == 'direct':\n",
    "            return inputs[:, self.perm], torch.zeros(\n",
    "                inputs.size(0), 1, device=inputs.device)\n",
    "        else:\n",
    "            return inputs[:, self.inv_perm], torch.zeros(\n",
    "                inputs.size(0), 1, device=inputs.device)\n",
    "        \n",
    "        \n",
    "class FlowSequential(nn.Sequential):\n",
    "    \"\"\" A sequential container for flows.\n",
    "    In addition to a forward pass it implements a backward pass and\n",
    "    computes log jacobians.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, inputs, cond_inputs=None, mode='direct', logdets=None):\n",
    "        \"\"\" Performs a forward or backward pass for flow modules.\n",
    "        Args:\n",
    "            inputs: a tuple of inputs and logdets\n",
    "            mode: to run direct computation or inverse\n",
    "        \"\"\"\n",
    "        self.num_inputs = inputs.size(-1)\n",
    "\n",
    "        if logdets is None:\n",
    "            logdets = torch.zeros(inputs.size(0), 1, device=inputs.device)\n",
    "\n",
    "        assert mode in ['direct', 'inverse']\n",
    "        if mode == 'direct':\n",
    "            for module in self._modules.values():\n",
    "                inputs, logdet = module(inputs, cond_inputs, mode)\n",
    "                logdets += logdet\n",
    "        else:\n",
    "            for module in reversed(self._modules.values()):\n",
    "                inputs, logdet = module(inputs, cond_inputs, mode)\n",
    "                logdets += logdet\n",
    "\n",
    "        return inputs, logdets\n",
    "\n",
    "    def log_probs(self, inputs, cond_inputs = None):\n",
    "        u, log_jacob = self(inputs, cond_inputs)\n",
    "        log_probs = (-0.5 * u.pow(2) - 0.5 * math.log(2 * math.pi)).sum(\n",
    "            -1, keepdim=True)\n",
    "        return (log_probs + log_jacob).sum(-1, keepdim=True)\n",
    "\n",
    "    def sample(self, num_samples=None, noise=None, cond_inputs=None):\n",
    "        if noise is None:\n",
    "            noise = torch.Tensor(num_samples, self.num_inputs).normal_()\n",
    "        device = next(self.parameters()).device\n",
    "        noise = noise.to(device)\n",
    "        if cond_inputs is not None:\n",
    "            cond_inputs = cond_inputs.to(device)\n",
    "        samples = self.forward(noise, cond_inputs, mode='inverse')[0]\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac9d981-46ce-42c6-a9b7-7c7c6d9a9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 10\n",
    "lr = 0.0001\n",
    "num_inputs = n_dim\n",
    "cond = True\n",
    "num_cond_inputs = 1\n",
    "num_hidden = 128\n",
    "act = 'relu'\n",
    "num_blocks = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a910a6e-29ff-4452-b542-e9963b660023",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = torch.from_numpy(data.astype(np.float32))\n",
    "train_labels = torch.from_numpy(slopes.astype(np.float32))\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    train_tensor, train_labels,\n",
    "    )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b259ac72-2088-4fc7-b027-abfa6361f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = []\n",
    "for _ in range(num_blocks):\n",
    "    modules += [\n",
    "        fnn.MADE(num_inputs, num_hidden, num_cond_inputs, act=act),\n",
    "        fnn.BatchNormFlow(num_inputs),\n",
    "        fnn.Reverse(num_inputs),\n",
    "        ]\n",
    "model = fnn.FlowSequential(*modules)\n",
    "\n",
    "for module in model.modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.orthogonal_(module.weight)\n",
    "        if hasattr(module, 'bias') and module.bias is not None:\n",
    "            module.bias.data.fill_(0)\n",
    "            \n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a9bae-1f1b-4430-a992-8dc646261569",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-6)\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "def train(epoch):\n",
    "    \n",
    "    global global_step\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    pbar = tqdm(total=len(train_loader.dataset))\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        if isinstance(data, list):\n",
    "            if len(data) > 1:\n",
    "                cond_data = data[1].float()\n",
    "                cond_data = cond_data.to(device)\n",
    "            else:\n",
    "                cond_data = None\n",
    "                \n",
    "            data = data[0]\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = -model.log_probs(data, cond_data).mean()\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pbar.update(data.size(0))\n",
    "        pbar.set_description(\n",
    "            f'Train, log lkl in nats: {-train_loss/(batch_idx+1):.6f}',\n",
    "            )\n",
    "        \n",
    "        global_step += 1\n",
    "        \n",
    "    pbar.close()\n",
    "    \n",
    "    for module in model.modules():\n",
    "        if isinstance(module, fnn.BatchNormFlow):\n",
    "            module.momentum = 0\n",
    "            \n",
    "    if cond:\n",
    "        with torch.no_grad():\n",
    "            model(\n",
    "                train_loader.dataset.tensors[0].to(data.device),\n",
    "                train_loader.dataset.tensors[1].to(data.device).float(),\n",
    "                )\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            model(\n",
    "                train_loader.dataset.tensors[0].to(data.device),\n",
    "                )\n",
    "    \n",
    "    for module in model.modules():\n",
    "        if isinstance(module, fnn.BatchNormFlow):\n",
    "            module.momentum = 1\n",
    "            \n",
    "    return train_loss / len(train_loader.dataset)\n",
    "            \n",
    "best_train_loss = float('inf')\n",
    "best_train_epoch = 0\n",
    "best_model = model\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\nEpoch: {epoch}')\n",
    "    \n",
    "    train_loss = train(epoch)\n",
    "    \n",
    "#     if epoch - best_train_epoch >= 30:\n",
    "#         break\n",
    "        \n",
    "    if train_loss < best_train_loss:\n",
    "        best_train_epoch = epoch\n",
    "        best_train_loss = train_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        \n",
    "    print(\n",
    "        f'Best training epoch at {best_train_epoch}: ' \\\n",
    "        f'Average log lkl in nats: {-best_train_loss:.6f}',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b21d8-5fb4-48f9-89fd-996b60d3cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    corner(\n",
    "        best_model.sample(10000, cond_inputs=train_dataset[10000][1]).numpy()\n",
    "        );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d42a340-e127-42d1-9d05-293f7780c250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shallow",
   "language": "python",
   "name": "shallow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8454efe-216d-4db3-a0b3-fc5f788d8a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from corner import corner\n",
    "import torch\n",
    "from torch import optim\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8076b2fd-ff4d-47f8-b1fe-17a8d0fcacf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from nflows.distributions.normal import StandardNormal\n",
    "from nflows.flows.base import Flow\n",
    "from nflows.transforms.autoregressive import MaskedAffineAutoregressiveTransform\n",
    "from nflows.transforms.base import CompositeTransform\n",
    "from nflows.transforms.normalization import BatchNorm\n",
    "from nflows.transforms.permutations import RandomPermutation, ReversePermutation\n",
    "from nflows.transforms.nonlinearities import Tanh\n",
    "from nflows.transforms.standard import PointwiseAffineTransform\n",
    "from nflows.transforms.base import InverseTransform\n",
    "\n",
    "\n",
    "class MaskedAutoregressiveFlow(Flow):\n",
    "    \"\"\"An autoregressive flow that uses affine transforms with masking.\n",
    "    Reference:\n",
    "    > G. Papamakarios et al., Masked Autoregressive Flow for Density Estimation,\n",
    "    > Advances in Neural Information Processing Systems, 2017.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        features,\n",
    "        hidden_features,\n",
    "        context_features=None,\n",
    "        num_layers=1,\n",
    "        num_blocks_per_layer=1,\n",
    "        use_residual_blocks=True,\n",
    "        use_random_masks=False,\n",
    "        use_random_permutations=False,\n",
    "        activation=F.relu,\n",
    "        dropout_probability=0.,\n",
    "        batch_norm_within_layers=False,\n",
    "        batch_norm_between_layers=False,\n",
    "    ):\n",
    "\n",
    "        if use_random_permutations:\n",
    "            permutation_constructor = RandomPermutation\n",
    "        else:\n",
    "            permutation_constructor = ReversePermutation\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            # InverseTransform(PointwiseAffineTransform(shift=.5, scale=.5)),\n",
    "            PointwiseAffineTransform(shift=-1., scale=2.)\n",
    "            )\n",
    "        layers.append(InverseTransform(Tanh()))\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(permutation_constructor(features))\n",
    "            layers.append(\n",
    "                MaskedAffineAutoregressiveTransform(\n",
    "                    features=features,\n",
    "                    hidden_features=hidden_features,\n",
    "                    context_features=context_features,\n",
    "                    num_blocks=num_blocks_per_layer,\n",
    "                    use_residual_blocks=use_residual_blocks,\n",
    "                    random_mask=use_random_masks,\n",
    "                    activation=activation,\n",
    "                    dropout_probability=dropout_probability,\n",
    "                    use_batch_norm=batch_norm_within_layers,\n",
    "                )\n",
    "            )\n",
    "            if batch_norm_between_layers:\n",
    "                layers.append(BatchNorm(features))\n",
    "\n",
    "        super().__init__(\n",
    "            transform=CompositeTransform(layers),\n",
    "            distribution=StandardNormal([features]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ec7e9a-8563-4989-8079-5d3dd3b6ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb44fd9-7d7b-46bb-aaef-be59d2c898e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerlaw(x, slope, lo, hi):\n",
    "    \n",
    "    return (\n",
    "        (x >= lo) * (x <= hi)\n",
    "        * x**slope\n",
    "        * (slope+1) / (hi**(slope+1) - lo**(slope+1))\n",
    "        )\n",
    "\n",
    "def sample_powerlaw(n_samples, slope, lo, hi):\n",
    "    \n",
    "    x = np.random.uniform(size=n_samples)\n",
    "    \n",
    "    return (lo**(slope+1) + x * (hi**(slope+1) - lo**(slope+1)))**(1/(slope+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ece374-f1db-4772-be55-d2fd89ef8e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_slopes = 10\n",
    "n_train = 10000\n",
    "n_valid = 10000\n",
    "\n",
    "slopes_lo = 0\n",
    "slopes_hi = 5\n",
    "data_lo = 0\n",
    "data_hi = 1\n",
    "n_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3fc56e-4bc6-4b55-9893-46a2eeb36d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes_train = np.random.uniform(\n",
    "    low=slopes_lo, high=slopes_hi, size=n_slopes,\n",
    "    )\n",
    "slopes_valid = np.random.uniform(\n",
    "    low=slopes_lo, high=slopes_hi, size=n_slopes,\n",
    "    )\n",
    "\n",
    "data_train = sample_powerlaw(\n",
    "    (n_slopes, n_train, n_dim),\n",
    "    slopes_train[:, None, None],\n",
    "    data_lo,\n",
    "    data_hi,\n",
    "    )\n",
    "data_valid = sample_powerlaw(\n",
    "    (n_slopes, n_valid, n_dim),\n",
    "    slopes_valid[:, None, None],\n",
    "    data_lo,\n",
    "    data_hi,\n",
    "    )\n",
    "\n",
    "slopes_train.shape, slopes_valid.shape, data_train.shape, data_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057307b-247b-4e09-a4bc-f27c36c5e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "corner(data_train[i]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24020316-7c86-4640-815d-6aa6a3a4bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes_train = np.repeat(slopes_train[:, None, None], n_train, axis=1)\n",
    "slopes_valid = np.repeat(slopes_valid[:, None, None], n_valid, axis=1)\n",
    "\n",
    "slopes_train.shape, slopes_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa560bb-f6cb-4e61-a1b1-d09c6eb3708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes_train = slopes_train.reshape(-1, 1)\n",
    "slopes_valid = slopes_valid.reshape(-1, 1)\n",
    "data_train = data_train.reshape(-1, n_dim)\n",
    "data_valid = data_valid.reshape(-1, n_dim)\n",
    "\n",
    "slopes_train.shape, slopes_valid.shape, data_train.shape, data_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cafd149-336d-4e01-9c39-de1c26ed98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 2\n",
    "context_features = 1\n",
    "num_layers = 3\n",
    "hidden_features = 32\n",
    "num_blocks_per_layer = 2\n",
    "batch_norm_between_layers = True\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534bfd88-559b-42ae-b379-9440284d71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = MaskedAutoregressiveFlow(\n",
    "    features=features,\n",
    "    context_features=context_features,\n",
    "    num_layers=num_layers,\n",
    "    hidden_features=hidden_features,\n",
    "    num_blocks_per_layer=num_blocks_per_layer,\n",
    "    batch_norm_between_layers=batch_norm_between_layers,\n",
    "    )\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    flow.parameters(),\n",
    "    lr=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5397cb2-971d-4a76-9047-206ab72f15d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.train(False)\n",
    "corner(flow.sample(10000, [[0.]]).detach().numpy()[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f97346-c87d-4d89-91b3-cef690804050",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(\n",
    "        torch.as_tensor(data_train, dtype=torch.float32),\n",
    "        torch.as_tensor(slopes_train, dtype=torch.float32),\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    )\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(\n",
    "        torch.as_tensor(data_valid, dtype=torch.float32),\n",
    "        torch.as_tensor(slopes_valid, dtype=torch.float32),\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ade4c-8e96-4f57-ae22-f492f9d1bea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = np.inf\n",
    "best_epoch = 0\n",
    "best_flow = flow\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(epoch)\n",
    "    \n",
    "    flow.train(True)\n",
    "    train_loss = 0.\n",
    "    for batch_idx, data in enumerate(tqdm(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        loss = -flow.log_prob(inputs=data[0], context=data[1]).mean()\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    flow.train(False)\n",
    "    valid_loss = 0.\n",
    "    for batch_idx, data in enumerate(tqdm(valid_loader)):\n",
    "        loss = -flow.log_prob(inputs=data[0], context=data[1]).mean()\n",
    "        valid_loss += loss.item()\n",
    "        \n",
    "    if valid_loss < best_loss:\n",
    "        print('Validation loss improved')\n",
    "        best_loss = valid_loss\n",
    "        best_epoch = epoch\n",
    "        best_flow = copy.deepcopy(flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d043c1-2f2a-42d4-8437-365a30250ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_flow.train(False)\n",
    "corner(best_flow.sample(10000, [[1.]]).detach().numpy()[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a146c04-d148-44ff-b8f5-07bbbf189885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shallow",
   "language": "python",
   "name": "shallow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

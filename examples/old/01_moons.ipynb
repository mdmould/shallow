{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "870e3519-1387-4219-a07d-66d8922f0cef",
   "metadata": {},
   "source": [
    "## Common Bijectors\n",
    "\n",
    "The choice of bijector functions is a fast changing area. I will thus only mention a few. You can of course use any bijective function or matrix, but these become inefficient at high-dimension due to the Jacobian calculation. One class of efficient bijectors are autoregressive bijectors. These have triangular Jacobians because each output dimension can only depend on the dimensions with a lower index. There are two variants: masked autoregressive flows (MAF){cite}`papamakarios2017masked` and inverse autoregressive flows (IAF) {cite}`kingma2016improved`. MAFs are efficient at training and computing probabilities, but are slow for sampling from $P(x)$. IAFs are slow at training and computing probabilities but efficient for sampling. Wavenets combine the advantages of both {cite}`kim2018flowavenet`. I'll mention one other common bijector which is not autoregressive: real non-volume preserving (RealNVPs) {cite}`dinh2016density`. RealNVPs are less expressive than IAFs/MAFs, meaning they have trouble replicating complex distributions, but are efficient at all three tasks: training, sampling, and computing probabilities. Another interesting variant is the Glow bijector,which is able to expand the rank of the normalizing flow, for example going from a matrix to an RGB image {cite}`das2019dimensionality`. What are the equations for all these bijectors? Most are variants of standard neural network layers but with special rules about which outputs depend on which inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e85c7c-151b-4640-91b3-535036a542bc",
   "metadata": {},
   "source": [
    "### Z Distribution\n",
    "\n",
    "Our Z distribution should always be as simple as possible. I'll create a 2D Gaussian with unit variance, no covariance, and centered at 0. I'll be using the tensorflow probability package for this example. The key new concept is that we organize our tensors that were *sampled* from a probability distribution in a specific way. We, by convention, make the first axes be the **sample** shape, the second axes be the **batch** shape, and the final axes be the **event** shape. The sample shape is the number of times we sampled from our distribution. It is a *shape* and not a single dimension because occasionally you'll want to organize your samples into some shape. The batch shape is a result of possibly multiple distributions batched together. For example, you might have 2 Gaussians, instead of a single 2D Gaussian. Finally, the event shape is the shape of a single sample from the distribution. This is overly complicated for our example, but you should be able to read information about the distribution now by understanding this nomenclature. You can find a tutorial on these [shapes here](https://www.tensorflow.org/probability/examples/Understanding_TensorFlow_Distributions_Shapes) and more tutorials on [tensorflow probability here](https://www.tensorflow.org/probability/examples/A_Tour_of_TensorFlow_Probability). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e591628-16db-4a67-ad62-11a689bc482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "import sklearn.datasets as datasets\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b92120-7da6-4e64-8200-bc929b64d7c0",
   "metadata": {},
   "source": [
    "## Target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d128b4-0f72-4ede-9612-6019fdff1f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "n_dim = 2\n",
    "data, _ = datasets.make_moons(n_samples, noise=.05)\n",
    "\n",
    "print(data.shape, _.shape)\n",
    "\n",
    "plt.scatter(*data.T);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bccfa0-31b2-4471-b12e-bbc9ea26a44d",
   "metadata": {},
   "source": [
    "## Normalizing flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f09026e-1caf-464d-9821-8077f09447cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_layers = 3\n",
    "# bijectors = []\n",
    "\n",
    "# for i in range(n_layers):\n",
    "#     bijectors.append(\n",
    "#         tfb.MaskedAutoregressiveFlow(tfb.AutoregressiveNetwork(\n",
    "#             params=n_dim,\n",
    "#             hidden_units=[128, 128],\n",
    "#             activation='relu',\n",
    "#             #activation=tfa.activations.rrelu,\n",
    "#             ))\n",
    "#         )\n",
    "#     bijectors.append(tfb.Permute([1, 0]))\n",
    "    \n",
    "# bijector = tfb.Chain(bijectors)\n",
    "# trans_dist = tfd.TransformedDistribution(z_dist, bijector=bijector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8637c0d7-b0e8-4204-8488-5bd5aa4c8a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_flows = 10\n",
    "n_layers = 1\n",
    "n_neurons = 1024\n",
    "\n",
    "# distribution = tfd.Sample(\n",
    "#     distribution=tfd.MultivariateNormalDiag(loc=[0]*n_dim),\n",
    "#     )\n",
    "distribution = tfd.MultivariateNormalDiag(loc=[0]*n_dim)\n",
    "\n",
    "bijectors = []\n",
    "for _ in range(n_flows):\n",
    "    bijectors.append(\n",
    "        tfb.MaskedAutoregressiveFlow(tfb.AutoregressiveNetwork(\n",
    "            params=n_dim,\n",
    "            hidden_units=[n_neurons]*n_layers,\n",
    "            activation='relu',\n",
    "            #activation=tfa.activations.rrelu,\n",
    "            ))\n",
    "        )\n",
    "    bijectors.append(tfb.Permute([1,0]))\n",
    "    \n",
    "bijector = tfb.Chain(bijectors)\n",
    "\n",
    "nf = tfd.TransformedDistribution(\n",
    "    distribution=distribution,\n",
    "    bijector=bijector,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d6384-379b-4ec8-a161-6bb9146de8e8",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a053fd-042e-47ee-a43b-61efcead050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.Input(shape=[n_dim], dtype=tf.float32)\n",
    "log_prob = nf.log_prob(x)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=x,\n",
    "    outputs=log_prob,\n",
    "    )\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=lambda _, log_prob: -log_prob,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302dba8c-350e-48dc-9db4-3e4da6429c63",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "batch_size = 32\n",
    "steps_per_epoch = n_samples // batch_size\n",
    "\n",
    "result = model.fit(\n",
    "    x=data, y=np.zeros(n_samples),\n",
    "    epochs=epochs, \n",
    "    batch_size=batch_size,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    shuffle=True,\n",
    "    verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e481f5a-3e84-48ca-8876-4479809560b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(result.history['loss']);\n",
    "plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9cc593-c0fd-412e-bc31-92d6622dd815",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b871326c-d05f-41ae-afc9-14eff40b0a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_samples = nf.sample(n_samples)\n",
    "\n",
    "plt.scatter(*z_samples.numpy().T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8641c1-48c8-416b-86e4-3448c9e5c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_points = np.linspace(-2.5, 2.5, 200)\n",
    "z1, z2 = np.meshgrid(z_points, z_points)\n",
    "z_grid = np.concatenate(\n",
    "    [z1.reshape(-1, 1), z2.reshape(-1, 1)], axis=1,\n",
    "    )\n",
    "\n",
    "probs = np.exp(nf.log_prob(z_grid))                      \n",
    "\n",
    "plt.imshow(\n",
    "    probs.reshape(z1.shape),\n",
    "    aspect='equal',\n",
    "    origin='lower',\n",
    "    extent=[-2.5, 2.5, -2.5, 2.5],\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda70bae-9eb8-451b-85e7-60c98c87f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = gaussian_kde(data.T)(z_grid.T)\n",
    "\n",
    "plt.imshow(\n",
    "    kde.reshape(z1.shape),\n",
    "    aspect='equal',\n",
    "    origin='lower',\n",
    "    extent=[-2.5, 2.5, -2.5, 2.5],\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3d235f-00fb-45c5-828b-a10a6a7dc04d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probflow",
   "language": "python",
   "name": "probflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
